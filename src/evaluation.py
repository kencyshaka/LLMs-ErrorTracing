import abc

import numpy as np
import wandb
import os
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import nltk
from nltk import ngrams
from sklearn.metrics import hamming_loss, accuracy_score, precision_score, recall_score, fbeta_score, f1_score, \
    multilabel_confusion_matrix, roc_auc_score
from matplotlib.backends.backend_pdf import PdfPages
from data_loader import get_errorID_embeddings
from evaluator.CodeBLEU import calc_code_bleu


def performance_granular(df, error_df, save_result, config):
    scores = {}
    first_scores = {}
    error_indices = error_df.set_index('key')['value'].to_dict()
    score_names_multilabel = ['Hamming_Loss', 'F1_Score_Weighted', 'F1_Score_Beta_Weighted', 'Precision_Weighted',
                              'Recall_Weighted', 'Accuracy']
    score_names_binary = ['Hamming_Loss', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'F1_Score_Beta']

    preds, gts, first_preds, first_gts = get_attempted_question(df, error_indices, config)
    predictions = df['error_class'].tolist()
    ground_truth = df['target_errors'].tolist()

    prediction_vector = [get_errorID_embeddings(error_indices, error, config) for error in predictions]
    gts_vector = [get_errorID_embeddings(error_indices, error, config) for error in ground_truth]

    ## calculate per question
    first_total_gts = []
    first_total_preds = []

    for j in range(config.questions):
        print("problem", j, len(preds[j]))

        metrics_values = evaluate_multilabel_classification(gts[j], preds[j])
        first_metrics_values = evaluate_multilabel_classification(first_gts[j], first_preds[j])

        scores[j] = extract_scores(metrics_values, score_names_multilabel)
        first_scores[j] = extract_scores(first_metrics_values, score_names_multilabel)

        print_metrics_multilabel(' overall problem ' + str(j), metrics_values)
        print_metrics_multilabel(' first problem   ' + str(j), first_metrics_values)

        first_total_gts.extend(first_gts[j])
        first_total_preds.extend(first_preds[j])

    print("************ overall performance *****************")

    # the overall prediction of errors
    overall_metrics_values = evaluate_multilabel_classification(gts_vector, prediction_vector)
    overall_first_metrics_values = evaluate_multilabel_classification(first_total_gts, first_total_preds)

    print_metrics_multilabel('overall  :', overall_metrics_values)
    print_metrics_multilabel('First    :', overall_first_metrics_values)

    first_total_scores = extract_scores(overall_first_metrics_values, score_names_multilabel)
    overall_total_scores = extract_scores(overall_metrics_values, score_names_multilabel)

    precision_per_class, recall_per_class, f1_per_class = evaluate_per_error(gts_vector, prediction_vector)
    error_scores = [precision_per_class, recall_per_class, f1_per_class]

    # the overall prediction of the pass/fail bottom up approach.
    pass_fail_preds, pass_fail_gts = get_predictions_for_pass_fail(prediction_vector, gts_vector)
    first_total_preds_pass, first_total_gts_pass = get_predictions_for_pass_fail(first_total_preds, first_total_gts)

    overall_metrics_values_pass = evaluate_binary_classification(pass_fail_gts, pass_fail_preds)
    overall_first_metrics_values_pass = evaluate_binary_classification(first_total_gts_pass, first_total_preds_pass)
    first_total_scores_pass = extract_scores(overall_first_metrics_values_pass, score_names_binary)
    overall_total_scores_pass = extract_scores(overall_metrics_values_pass, score_names_binary)
    print_metrics_binary('over pass:', overall_metrics_values_pass)
    print_metrics_binary('First pas:', overall_first_metrics_values_pass)

    # calculating the codebleu values

    empty_code_count = df[df['generated_code'] == ""].shape[0] # Count how many rows have an empty 'code' column
    print("*********** Number of code not generated by LLM: ", empty_code_count)

    df_clean = df[df['generated_code'] != ""].copy()  # Create a copy of the DataFrame and
    removed_rows_count = df.shape[0] - df_clean.shape[0]  # remove rows where 'code' column is empty
    print(f"Number of rows removed: {removed_rows_count}")

    ground_truth_codes = df_clean['target_code'].tolist()
    generated_codes = df_clean['generated_code'].tolist()
    codebleu_score, detailed_codebleu_score = compute_code_bleu(ground_truth_codes, generated_codes)

    print("**************************** Done, logging info ***********************************************************")

    wandb.log(
        {"First_F1": first_total_scores[1],
         "Overall_F1": overall_total_scores[1],
         "First_F1Beta": first_total_scores[2],
         "Overall_F1Beta": overall_total_scores[2],
         "First_passF1": first_total_scores_pass[1],
         "Overall_passF1": overall_total_scores_pass[1],
         "CodeBleu": codebleu_score,
         "CodeBleu_detailed": detailed_codebleu_score,
        # 'dist_1': Distinct_N(1),
        # 'dist_2': Distinct_N(2),
        # 'dist_3': Distinct_N(3),
         })

    # logging the overall table performance
    columns = ["category", "hm", "f1", "f1Beta", "recall", "precision", "acc"]

    table_data = []
    table_data.append(["first"] + first_total_scores)
    table_data.append(["overall"] + overall_total_scores)
    table_data.append(["first_pass",first_total_scores_pass[0], first_total_scores_pass[4], first_total_scores_pass[5], first_total_scores_pass[3], first_total_scores_pass[2], first_total_scores_pass[1]])
    table_data.append(["overall_pass", overall_total_scores_pass[0], overall_total_scores_pass[4], overall_total_scores_pass[5], overall_total_scores_pass[3], overall_total_scores_pass[2], overall_total_scores_pass[1]])

    table = wandb.Table(data=table_data, columns=columns)
    wandb.log({"Overall performance": table})

    # logging per problem
    table_data = []
    for id in range(config.questions):
        problem_data = ["Problem_" + str(id + 1)] + first_scores[id]
        table_data.append(problem_data)

    table = wandb.Table(data=table_data, columns=columns)
    wandb.log({"Avg First per problem": table})

    for id in range(config.questions):
        problem_data = ["Problem_" + str(id + 1)] + scores[id]
        table_data.append(problem_data)

    table = wandb.Table(data=table_data, columns=columns)
    wandb.log({"Avg Overall per problem": table})

    # logging per error
    error_columns = ["errorIDs", "Precision", "Recall", "F1score"]
    table_data = []
    for i in range(config.top_error_size):
        error_data = ["ErrorID_" + str(i + 1), error_scores[0][i], error_scores[1][i], error_scores[2][i]]
        table_data.append(error_data)

    table = wandb.Table(data=table_data, columns=error_columns)
    wandb.log({"Avg per error": table})

    # plotting and logging confusion matrix
    labels_first = ['ErrorID ' + str(i) for i in range(config.top_error_size)]
    cm_filename = os.path.join(save_result, "confusion_matrices.pdf")
    confusion_matrix = overall_metrics_values['confusion_matrix']
    plot_confusion_matrix(confusion_matrix, labels_first, cm_filename)

    print("**************************** Done ***********************************************************")


def plot_confusion_matrix(confusion_matrices: object, labels_first: object, filename: object) -> object:
    # Create subplots for the first subset
    fig, axes = plt.subplots(2, 5, figsize=(16, 8))
    axes = axes.ravel()

    print("confusion matrx file name ", filename)
    # Create a PDF file to save the confusion matrices
    pdf_pages = PdfPages(filename)

    # Plot and save the confusion matrices for the first subset
    for i in range(len(labels_first)):
        # Plot the confusion matrix

        axes[i].imshow(confusion_matrices[i], interpolation='nearest', cmap=plt.cm.Blues)
        axes[i].set_title('Confusion Matrix - ' + labels_first[i])
        axes[i].set_xticks([0, 1])
        axes[i].set_yticks([0, 1])
        axes[i].set_xticklabels(['Negative', 'Positive'])
        axes[i].set_yticklabels(['Negative', 'Positive'])
        axes[i].set_xlabel('Predicted')
        axes[i].set_ylabel('True')

        # Add text annotations to indicate correct/incorrect predictions
        for row in range(2):
            for col in range(2):
                # Determine the text color based on the cell value
                text_color = 'brown' if confusion_matrices[i][row, col] > confusion_matrices[i][
                    col, col] / 2 else 'black'

                # Add the text annotation to the cell
                # print(confusion_matrices[i][row, col])
                axes[i].text(col, row, confusion_matrices[i][row, col], ha='center', va='center', color=text_color)

    # Display the confusion matrix
    plt.tight_layout()
    # plt.show()

    # Save the figure to the PDF file
    pdf_pages.savefig(fig)

    pdf_pages.close()
    plt.close()

    # Log the confusion matrix to Weights & Biases

    wandb.log({labels_first[i]: wandb.Image(fig)})

    # Clear the current figure
    plt.clf()


def get_attempted_question(df, error_indices, config):
    # Initialize the dictionaries
    preds = {k: [] for k in range(config.questions)}
    gts = {k: [] for k in range(config.questions)}
    first_preds = {k: [] for k in range(config.questions)}
    first_gts = {k: [] for k in range(config.questions)}

    # Populate the dictionaries
    for question in range(config.questions):
        prediction = df[df['problem_id'] == question]['error_class'].tolist()
        ground_truth = df[df['problem_id'] == question]['target_errors'].tolist()

        print("prediction",prediction)
        preds[question] = [get_errorID_embeddings(error_indices, error, config) for error in prediction]
        gts[question] = [get_errorID_embeddings(error_indices, error, config) for error in ground_truth]

        first_prediction = df[df['problem_id'] == question].groupby('user_id').first()['error_class'].tolist()
        first_ground_truth = df[df['problem_id'] == question].groupby('user_id').first()['target_errors'].tolist()

        first_preds[question] = [get_errorID_embeddings(error_indices, error, config) for error in first_prediction]
        first_gts[question] = [get_errorID_embeddings(error_indices, error, config) for error in first_ground_truth]

    # tools.display_dataframe_to_user(name="Predictions and Ground Truths Data", dataframe=df)
    return preds, gts, first_preds, first_gts


def print_metrics_multilabel(prefix, metrics_values):
    output = (
        f'{prefix} hamming: {metrics_values["Hamming_Loss"]}'
        f' f1: {metrics_values["F1_Score_Weighted"]}'
        f' f1_beta: {metrics_values["F1_Score_Beta_Weighted"]}'
        f' precision: {metrics_values["Precision_Weighted"]}'
        f' recall: {metrics_values["Recall_Weighted"]}'
        f' f1_macro: {metrics_values["F1_Score_Macro"]}'
        f' f1_beta_macro: {metrics_values["F1_Score_Beta_Macro"]}'
        f' precision_macro: {metrics_values["Precision_Macro"]}'
        f' recall_macro: {metrics_values["Recall_Macro"]}'
        f' sub_acc: {metrics_values["Accuracy"]}'
    )
    print(output)


def print_metrics_binary(prefix, metrics_values):
    output = (
        f'{prefix} hamming: {metrics_values["Hamming_Loss"]}'
        f' f1: {metrics_values["F1_Score"]}'
        f' f1_beta: {metrics_values["F1_Score_Beta"]}'
        f' precision: {metrics_values["Precision"]}'
        f' recall: {metrics_values["Recall"]}'
        f' sub_acc: {metrics_values["Accuracy"]}'
    )
    print(output)


def extract_scores(metrics_values, score_names):
    return [metrics_values[name] for name in score_names]


def evaluate_multilabel_classification(y_true, y_pred):

    # Calculate subset accuracy
    accuracy = accuracy_score(y_true, y_pred)

    # Calculate Hamming Loss
    hamming = hamming_loss(y_true, y_pred)

    # Calculate weighted-averaged precision, recall, and F1 score
    weighted_precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)  # , zero_division=1
    weighted_recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    weighted_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    weighted_f1_beta = fbeta_score(y_true, y_pred, average='weighted', beta=0.5, zero_division=0)

    # Calculate macro-averaged precision, recall, and F1 score
    macro_precision = precision_score(y_true, y_pred, average='macro', zero_division=0.0)
    macro_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
    macro_f1_beta = fbeta_score(y_true, y_pred, average='macro', beta=0.5, zero_division=0)

    # calculate multilabel_confusion_matrix
    cm = multilabel_confusion_matrix(y_true, y_pred)

    # Create a dictionary to store the results
    metrics = {
        'Hamming_Loss': hamming,
        'Precision_Macro': macro_precision,
        'Precision_Weighted': weighted_precision,
        'Recall_Macro': macro_recall,
        'Recall_Weighted': weighted_recall,
        'F1_Score_Beta_Macro': macro_f1_beta,
        'F1_Score_Macro': macro_f1,
        'F1_Score_Weighted': weighted_f1,
        'F1_Score_Beta_Weighted': weighted_f1_beta,
        'Accuracy': accuracy,
        'confusion_matrix': cm,
    }

    return metrics


def evaluate_binary_classification(y_true, y_pred):
    y_pred_thresholded = y_pred

    # Calculate AUC
    try:
        accuracy = roc_auc_score(y_true, y_pred)
    except ValueError:
        accuracy = 0.5

    # Calculate Hamming Loss
    hamming = hamming_loss(y_true, y_pred_thresholded)

    # Calculate weighted-averaged precision, recall, and F1 score
    precision = precision_score(y_true, y_pred_thresholded, zero_division=0.0)  # , zero_division=1
    recall = recall_score(y_true, y_pred_thresholded, zero_division=0.0)
    f1 = f1_score(y_true, y_pred_thresholded, zero_division=0.0)
    f1_beta = fbeta_score(y_true, y_pred_thresholded, zero_division=0.0, beta=0.5)

    # calculate multilabel_confusion_matrix
    cm = multilabel_confusion_matrix(y_true, y_pred_thresholded)

    # Create a dictionary to store the results
    metrics = {
        'Hamming_Loss': hamming,
        'Precision': precision,
        'Recall': recall,
        'F1_Score': f1,
        'F1_Score_Beta': f1_beta,
        'Accuracy': accuracy,
        'confusion_matrix': cm,
    }

    return metrics


def evaluate_per_error(ground_truth, prediction):
    # Initialize arrays to store the results per class
    precision_per_class = []
    recall_per_class = []
    f1_per_class = []

    # Calculate precision, recall, and F1 score per class
    for i in range(len(ground_truth[0])):
        class_true = [ground_truth[j][i] for j in range(len(ground_truth))]
        class_pred = [prediction[j][i] for j in range(len(prediction))]

        precision = precision_score(class_true, class_pred, zero_division=0)
        recall = recall_score(class_true, class_pred)
        f1 = f1_score(class_true, class_pred)

        precision_per_class.append(precision)
        recall_per_class.append(recall)
        f1_per_class.append(f1)

    # Print the results per class
    for i in range(len(precision_per_class)):
        print(f"Class {i}:")
        print(f"Precision: {precision_per_class[i]:.3f}")
        print(f"Recall: {recall_per_class[i]:.3f}")
        print(f"F1-score: {f1_per_class[i]:.3f}")

    return precision_per_class, recall_per_class, f1_per_class


def get_predictions_for_pass_fail(original_pred, original_gt):
    preds = []
    gts = []
    # length = len(original_gt)

    for i in range(len(original_gt)):  # 0.0 = no errors(pass) 1.0 = committed error(s)
        if original_pred[i][0] == 1.0:
            preds.append(0.0)    # student has passed
        else:
            preds.append(1.0)    # student has failed i.e presence of errors

        if original_gt[i][0] == 1.0:
            gts.append(0.0)
        else:
            gts.append(1.0)

    return preds, gts


def compute_code_bleu(ground_truth_codes, generated_codes):
    params = '0.25,0.25,0.25,0.25'
    lang = 'java'
    codebleu_score, detailed_codebleu_score = 0, 0
    # calc_code_bleu.get_codebleu(pre_references=[ground_truth_codes], hypothesis=generated_codes, lang=lang, params=params)
    return codebleu_score, detailed_codebleu_score


class Metric():
    """
    Defines a text quality metric.
    """

    def get_name(self):
        return self.name

    @abc.abstractmethod
    def compute_metric(self, texts):
        pass


class Distinct_N(Metric):

    def __init__(self, n):
        """
        Distinct n-grams metrics. This is a sequence-level diversity metric.
        See https://www.aclweb.org/anthology/N16-1014 for more details.

        Args:
            n (int): n-grams
        """

        self.n = n
        self.name = f'Distinct_{n}'

    def compute_metric(self, texts):
        return self._distinct_ngrams(texts, self.n)

    def _distinct_ngrams(self, texts, n):
        total = 0.0
        for t in texts:
            try:
                tokens = nltk.tokenize.word_tokenize(t)
                n_distinct = len(set(ngrams(tokens, n)))
                total += n_distinct/ len(tokens)
            except:
                continue

        return total / len(texts)
